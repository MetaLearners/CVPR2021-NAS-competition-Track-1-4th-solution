{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CVPR2021 NAS track1 stage2 3rd Solution Illustration\n",
    "Brought to you by **纳萨力克地下大坟墓 (Great Tomb of Nazarick)** from **Meta_Learners**, **THUMNLab**, **Tsinghua University**\n",
    "\n",
    "**Team Member**: Chaoyu Guan (关超宇) (**Leader**), Yijian Qin (秦一鉴), Zhikun Wei (卫志坤), Zeyang Zhang (张泽阳), Zizhao Zhang (张紫昭), Xin Wang (王鑫)\n",
    "### Table of content\n",
    "- [Train](#I)\n",
    "- [Evaluate](#II)\n",
    "- [Tuning](#III)\n",
    "\n",
    "### <a id=\"I\">Part I. Train </a>\n",
    "Before training, we need to prepare dataset under `./data` folder. Please first download the cifar100 dataset from [competition website](https://aistudio.baidu.com/aistudio/datasetdetail/76994) and move it to `./data/cifar-100-python.tar.gz`, and download the submit 5w archs from [competition website](https://aistudio.baidu.com/aistudio/datasetdetail/73326) to `./data/Track1_final_archs.json`\n",
    "\n",
    "Then, we need to import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "import paddle.optimizer as opt\n",
    "from paddle.optimizer.lr import CosineAnnealingDecay, LinearWarmup\n",
    "\n",
    "from supernet.utils import seed_global, Dataset, str2arch, get_param\n",
    "from supernet.sample import Generator, strict_fair_sample, uniform_sample\n",
    "from supernet.super.supernet import Supernet\n",
    "from supernet.super import super_bn, super_conv, super_fc\n",
    "\n",
    "# BUG: there are some bugs when seeding paddlepaddle. Even the seed is set for paddlepaddle, the training procedure still has randomness.\n",
    "# you can run this notebook several times and you will get different supernet performance even when paddle.seed(0) is called inside seed_global.\n",
    "seed_global(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can build a supernet using shared conv, bn and fc module, and load the necessary data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supernet = Supernet(super_bn.BestBN, super_conv.BestConv, super_fc.BestFC)\n",
    "dataloader = Dataset().get_loader(batch_size=128, mode='train', num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train the supernet following paper [\"Universally Slimmable Networks and Improved Training Techniques\"](https://arxiv.org/abs/1903.05134). The basic idea is that, for one data batch, we randomly sample $n - 2$ architectures, together with two fixed architectures: The biggest architecture and the smallest architecture. For the biggest architecture, we use ground truth label of data batch for training. For the other architectures, we use knowledge distillation and use the biggest architecture as teacher to guide the training.\n",
    "\n",
    "The idea behind is that the biggest and smallest architectures represent the performance upper and lower bound of the space. Optimizing both will result in better overall performances.\n",
    "\n",
    "To get a better rank, we use basically the same training hyperparameters with each sub-architecture training procedure. And use channel-wise fair sampling method borrowed from [\"FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search\"](https://arxiv.org/abs/1907.01845) to sample the $n - 2$ architectures. The basic idea is that, for every $m\\times k$ architectures sampled, every channel in certain layer appears exactly $m$ times, where $k$ is the number of choices that layer can choose.\n",
    "\n",
    "**NOTE: The following cell will run about 10h to train the supernet on 1 Tesla V100.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "EPOCH = 300\n",
    "LR = 0.1\n",
    "WEIGHT_DECAY = 0.1     # we find that using weight decay 0 will result in better ranks. See chapter III. Tuning\n",
    "GRAD_CLIP = 5          # we find that using gradient clip will result in slightly better ranks. See chapter III. tuning\n",
    "SPACES = (\n",
    "    [[4, 8, 12, 16]] * 7\n",
    "    + [[4, 8, 12, 16, 20, 24, 28, 32]] * 6\n",
    "    + [[4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64]] * 7\n",
    ")\n",
    "N = 18                 # We choose n=18 instead of a small n in paper, which we find is more suitable in this competition. See chapter III. tuning\n",
    "SKIP_CONNECT = True    # We find that turning on skip connection will result in better ranks. See chapter III. tuning\n",
    "\n",
    "# build an architecture generator sampled according to channel-wise fair sampling\n",
    "generator = Generator(strict_fair_sample)\n",
    "max_arch = [max(x) for x in SPACES]\n",
    "min_arch = [min(x) for x in SPACES]\n",
    "\n",
    "# build the optimizer\n",
    "optimizer = opt.Momentum(\n",
    "    LinearWarmup(CosineAnnealingDecay(LR, EPOCH), 2000, 0.0, LR),\n",
    "    momentum=0.9,\n",
    "    parameters=supernet.parameters(),\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    grad_clip=None if GRAD_CLIP <= 0 else nn.ClipGradByGlobalNorm(GRAD_CLIP),\n",
    ")\n",
    "optimizer.clear_grad()\n",
    "\n",
    "for e in tqdm(range(EPOCH), desc=\"total\"):\n",
    "    for data in tqdm(dataloader, desc=f\"{e} epoch\"):\n",
    "        # sample archs\n",
    "        archs = [generator() for _ in range(N - 2)] + [min_arch]\n",
    "        \n",
    "        # use ground truth label to train the biggest model.\n",
    "        logit_big = supernet(data[0], max_arch, SKIP_CONNECT)\n",
    "        loss = F.cross_entropy(logit_big, data[1]) / N\n",
    "        loss.backward()\n",
    "\n",
    "        # use output of big model to knowledge distill other models\n",
    "        with paddle.no_grad():\n",
    "            distribution = F.softmax(logit_big).detach()\n",
    "        for arch in archs:\n",
    "            logit = supernet(data[0], arch, SKIP_CONNECT)\n",
    "            loss = F.cross_entropy(logit, distribution, soft_label=True) / N\n",
    "            loss.backward()\n",
    "\n",
    "        # update parameters of supernet\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        # update scheduler\n",
    "        if optimizer._learning_rate.last_epoch < optimizer._learning_rate.warmup_steps:\n",
    "            optimizer._learning_rate.step()\n",
    "        \n",
    "        # save models for this epoch\n",
    "        paddle.save(supernet.state_dict(), f'./saved_models/{e}-model.pdparams')\n",
    "\n",
    "    # update scheduler\n",
    "    if optimizer._learning_rate.last_epoch >= optimizer._learning_rate.warmup_steps:\n",
    "        optimizer._learning_rate.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can evaluate all the trained supernets derived under `./saved_models` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"II\">Part II. Evaluate </a>\n",
    "For evaluation, we simply extract the parameters (without running batch norm statistics) from supernet to form the parameters of every single model. The batch norm statistics are calculated on the fly using the data batch statistics from test set. We find that using this technique will result in better ranks. See [III. tuning](#III) for more details.\n",
    "\n",
    "We also find that using negative cross entropy loss of single model on test dataset will result in better ranks with ground truth accuracy of model.\n",
    "\n",
    "**NOTE: the cell below will cost about 8 hours running on one Tesla V100.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_CONNECT = True\n",
    "\n",
    "with paddle.no_grad():\n",
    "    # build model, load best parameters\n",
    "    supernet = Supernet(super_bn.BestBN, super_conv.BestConv, super_fc.BestFC)\n",
    "\n",
    "    '''\n",
    "    # use the line below to generate the 0.97122 result\n",
    "    # supernet.set_state_dict(paddle.load('./saved_models/model_97122.pdparams'))\n",
    "    \n",
    "    # use the line below to generate the 0.97131 result\n",
    "    # supernet.set_state_dict(paddle.load('./saved_models/model_97131.pdparams'))\n",
    "    \n",
    "    # the 97131 and 97122 is two models reruned following the notebook instruction above\n",
    "    '''\n",
    "    supernet.set_state_dict(paddle.load('./saved_models/255-model.pdparams'))\n",
    "\n",
    "    supernet.eval()\n",
    "    dataloader = Dataset().get_loader(batch_size=512, mode='test', num_workers=4)\n",
    "\n",
    "    architecture = json.load(open('./data/Track1_final_archs.json', 'r'))\n",
    "    for key in tqdm(architecture, desc=\"all archs\"):\n",
    "        arch = architecture[key][\"arch\"]\n",
    "        arch = str2arch(arch)\n",
    "\n",
    "        loss = 0.\n",
    "        for data in tqdm(dataloader, desc=\"test dataset\"):\n",
    "            logit = supernet.inference(data[0], arch, SKIP_CONNECT)\n",
    "            loss += float(F.cross_entropy(logit, data[1], reduction='sum'))\n",
    "        architecture[key][\"acc\"] = - loss\n",
    "\n",
    "# normalize architecture accs\n",
    "max_acc = max([architecture[key]['acc'] for key in architecture])\n",
    "min_acc = min([architecture[key]['acc'] for key in architecture])\n",
    "\n",
    "for key in architecture:\n",
    "    architecture[key]['acc'] = (architecture[key]['acc'] - min_acc) / (max_acc - min_acc)\n",
    "\n",
    "# save the results\n",
    "json.dump(architecture, open(\"./saved_models/result_5w.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derived `./saved_models/result_5w.json` is the file we submit online.\n",
    "\n",
    "### <a id=\"III\">Part III. tuning</a>\n",
    "\n",
    "In this chapter, we will introduce what we have explored during this competition, and share you the findings.\n",
    "\n",
    "NOTE: since most of our results are derived using different version of codes, backends and structures, we __only provide the universal findings__ and __how to run these comparison experiments__. We do not report the exact rank scores since most of them are not runned under our newest version of codes.\n",
    "\n",
    "#### III.a Influence of different sharing strategies\n",
    "\n",
    "The first thing we've tested is how to share each parts of supernet. We can choose to share fc layer, share bn layer and share conv layer.\n",
    "- For fc layer, there are two choices:\n",
    "    - when not shared, every different last channel choice will use different fc layer. [__IndependentFC__]\n",
    "    - when shared, the fc layer with small channel number will be derived from the biggest fc layer by taking the smallest $c$ channels, where c is the small channel number. [__FullFC__]\n",
    "- For conv layer, there are 4 ways to do the share:\n",
    "    - Independent. For one layer, every choice of [in_c, out_c] will use independent kernel with shape [in_c, out_c]. [__IndependentConv__]\n",
    "    - Front share. For one layer, the conv op with the same input channel number will share the same kernel, wich the small kernel extracted from the shared kernel by choosing neurons with small ids. [__FrontShareConv__]\n",
    "    - End share. For one layer, the conv op with the same output channel number will share the same kernel, wich the small kernel extracted from the shared kernel by choosing neurons with small ids. [__EndShareConv__]\n",
    "    - Full share. Every op in one layer is a slice of the shared kernel by choosing the small ids. [__FullConv__]\n",
    "- For bn layer, we can also have 4 ways to do the share, similar to the situation in conv layer.\n",
    "    - Independent. Every choice of [in_c, out_c] will use independent bn ops of shape [out_c]. [__IndependentBN__]\n",
    "    - Front share. For one layer, the bn op with the same conv input channel number will be shared. [__FrontShareBN__]\n",
    "    - End share. For one layer, the bn op with the same conv output channel number will be shared. [__EndShareBN__]\n",
    "    - Full share. Use the same bn for every op choices. [__FullBN__]\n",
    "\n",
    "There are so many cross combinations, so we __do not iterate__ the whole space. Instead, we __start from the fully shared supernet (FullConv, FullBN, FullFC)__, and test whether the rank will improve when we change the share type of bn, conv and fc. Specially, we also test the basic combinations to use both _xxxConv_ and _xxxBN_, since they share the same logic when sharing.\n",
    "\n",
    "In total, we test the following combinations:\n",
    "1. FullConv, FullBN, FullFC\n",
    "2. IndependentConv, FullBN, FullFC\n",
    "3. FrontShareConv, FullBN, FullFC\n",
    "4. EndShareConv, FullBN, FullFC\n",
    "5. FullConv, IndependentBN, FullFC\n",
    "6. FullConv, FrontShareBN, FullFC\n",
    "7. FullConv, EndShareBN, FullFC\n",
    "8. FullConv, FullBN, IndependentFC\n",
    "9. FrontShareConv, FrontShareBN, FullFC\n",
    "10. EndShareConv, EndShareBN, FullFC\n",
    "11. IndependentConv, IndependentBN, FullFC\n",
    "\n",
    "You can use the following command to run every combination by replacing xxxconv xxbn xxfc to the corresponding classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m supernet.scripts.train --convclass xxxconv --bnclass xxbn --fcclass xxfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: __Fully shared supernet performs best__.\n",
    "\n",
    "#### III.b sample strategy\n",
    "\n",
    "The second exploration is how to sample architectures to train the supernet. We mainly explore 3 strategies.\n",
    "\n",
    "- uniform sample. Sample uniformly.\n",
    "- fair sample. Sample fairly following [\"FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search\"](https://arxiv.org/abs/1907.01845)\n",
    "- sample according to architecture parameter size. Architectures with bigger parameter size will have higher probability to be sampled.\n",
    "\n",
    "Using following commands to explore sample strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, generate the archs for `sampling according to architecture parameter size`\n",
    "archs = uniform_sample(1000000)\n",
    "arch_param = [get_param(a)[1] / 1000000 for a in archs]\n",
    "sums = sum(arch_param)\n",
    "arch_prob = [x / sums for x in arch_param]\n",
    "\n",
    "pickle.dump(archs, open('./data/arch_1m.bin', 'wb'))\n",
    "pickle.dump(arch_prob, open('./data/arch_1m_prob.bin', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m supernet.scripts.train --sample uniform\n",
    "!python -m supernet.scripts.train --sample fair\n",
    "!python -m supernet.scripts.train --sample fixed --train_arch ./data/arch_1m.bin --train_arch_prob ./data/arch_1m_prob.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: fair sample performs slightly better among all results.\n",
    "\n",
    "#### III.c Sandwich rule, knowledge distillation\n",
    "To test whether the sandwich rule and knowledge distillation make effects, we also explore the use of both. You can derive the supernet by running the following codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sandwich rule and knowledge distillation\n",
    "!python -m supernet.scripts.train --n 6 --kd --sandwich\n",
    "# use sandwich rule only\n",
    "!python -m supernet.scripts.train --n 6 --sandwich\n",
    "# do not use any\n",
    "!python -m supernet.scripts.train --n 6\n",
    "\n",
    "# use small n to rerun\n",
    "!python -m supernet.scripts.train --n 3 --kd --sandwich\n",
    "!python -m supernet.scripts.train --n 3 --sandwich\n",
    "!python -m supernet.scripts.train --n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: sandwich rule will only be effective when knowledge distillation is on and when $n$ is large(r than $5$). Sandwich rule + knowledge distillation can result in higher kendall score, while do not use both will result in higher pearson score.\n",
    "\n",
    "#### III.d Hyper parameters\n",
    "We also tune the lr, weight decay, gradient clip, training epoch, etc. You can use the following command to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m supernet.scripts.train --weight_decay xxx --lr xxx --epoch xxx --clip xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: weight decay should be 0, gradient clip = 5 is better. lr and epoch should remain the same with training ground truth: lr=0.1, epoch=300\n",
    "\n",
    "#### III.e N\n",
    "The main last hyper parameter during training we explore is N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m supernet.scripts.train --n xxx --kd --sandwich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: n=18 is the best\n",
    "\n",
    "#### III.f Evaluation\n",
    "\n",
    "We explore the evaluation methods. There are mainly two fields to explore.\n",
    "- The metric to use\n",
    "    - negative cross entropy loss [loss]\n",
    "    - top1 accuracy [acc]\n",
    "- The bn statistics to use\n",
    "    - those in supernet during training [0]\n",
    "    - calibrate them on training dataset [1]\n",
    "    - calibrate them on test dataset [2]\n",
    "    - calculate on the fly during testing [3]\n",
    "\n",
    "You can evaluate them using following commands:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m supernet.scripts.evaluate --path path/to/pdparams --path_to_arch path/to/arch --metric loss or acc --bn_mode 0 or 1 or 2 or 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: loss is better than acc, while calibrate bn performs similar with calculate on the fly. Directly using supernet statistics performs worst. For speed reason, we use 3 at last.\n",
    "\n",
    "#### III.g Others\n",
    "\n",
    "We've also test some other strategies, which is not so systematic or important, so we omit them in this notebook, you can refer to `./supernet/scripts/train` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
